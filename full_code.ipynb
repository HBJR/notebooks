{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8028128c-0e98-41ff-9239-6fca20e15d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from functools import reduce\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim import corpora\n",
    "from collections import namedtuple\n",
    "import logging\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a34ec40-3cf3-49b9-a39e-af0cd051cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to read in speech files line-by-line to deal with corrupted lines\n",
    "def file_load(file_path):\n",
    "    temp_dict = {\"speech_id\":[], \"speech\":[]}\n",
    "    with open(file_path, \"r\", encoding=\"ansi\") as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if i == 0: continue\n",
    "            ls = line.split(\"|\")\n",
    "            ls[1] = ls[1].replace(\"\\n\",\"\")\n",
    "            ls[1] = ls[1].replace(\"\\t\", \"\")\n",
    "            temp_dict[\"speech_id\"].append(int(ls[0]))\n",
    "            temp_dict[\"speech\"].append(ls[1])\n",
    "    return pd.DataFrame.from_dict(temp_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63016e-5261-4f8f-ba67-3ffd5ae80f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of congresses we want to retrieve speeches for\n",
    "congresses = [102, 103, 104, 105, 106, 107, 108]\n",
    "\n",
    "# initialise empty lists to hold dfs of speeches and speakers\n",
    "df_list = []\n",
    "speakers_df_list = []\n",
    "\n",
    "for congress in congresses: \n",
    "    # read in speeches and speakers for each congress in congresses\n",
    "    speeches = file_load(data_dir / f\"hein-daily/speeches_{congress}.txt\")\n",
    "    speakers = pd.read_csv(data_dir / f\"hein-daily/{congress}_SpeakerMap.txt\", delimiter=\"|\", encoding=\"ANSI\")\n",
    "    \n",
    "    # merge speeches and speakers together (so we have a speaker for each speech)\n",
    "    merged = speeches.merge(speakers, how=\"inner\", on=\"speech_id\").dropna()\n",
    "    \n",
    "    # filter to only speeches in the house and only voting representatives\n",
    "    house = merged[(merged[\"chamber\"] == \"H\") & (merged[\"nonvoting\"] == \"voting\")].copy()\n",
    "    house_speakers = speakers[(speakers[\"chamber\"] == \"H\") & (speakers[\"nonvoting\"] == \"voting\")].copy()\n",
    "    \n",
    "    house[\"congress\"] = str(congress)\n",
    "    \n",
    "    # get speech-invariante speaker characteristics and drop duplicates to get a dataframe with one observation of each speaker per Congress\n",
    "    house_speakers = house_speakers[[\"speakerid\", \"lastname\", \"firstname\", \"state\", \"district\"]]\n",
    "    house_speakers[\"speakerid\"] = house_speakers[\"speakerid\"].map(str)\n",
    "    house_speakers = house_speakers.drop_duplicates(ignore_index=True)\n",
    "    \n",
    "    # add to list of dfs\n",
    "    df_list.append(house)\n",
    "    speakers_df_list.append(speakers)\n",
    "\n",
    "# concatanate df_list together so we have one dataframe with all of the speeches of interest\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "speakers_df = pd.concat(speakers_df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be5c13-356c-410d-aefc-336ce0f55073",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(data_dir / \"house_102_to_108.txt\", sep=\"\\t\", index=False)\n",
    "speakers_df.to_csv(data_dir / \"speakers_102_to_108.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b3a0a-b2ac-47ac-821e-0344cbdad912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTRACTIONS AND PROCEDURAL WORDS LISTS DRAWN FROM RHEAULT AND COCHRANE (2020)\n",
    "\n",
    "# Define dictionary of contractions to map to whole words\n",
    "contractions_map = {\"you'd\": 'you would', \"he'd\": 'he would', \"she's\": 'she is', \"where'd\": 'where did', \"might've\": 'might have', \\\n",
    "                \"he'll\": 'he will', \"they'll\": 'they will',  \"mightn't\": 'might not', \"you'd've\": 'you would have', \"shan't\": 'shall not', \\\n",
    "                \"it'll\": 'it will', \"mayn't\": 'may not', \"couldn't\": 'could not', \"they'd\": 'they would', \"so've\": 'so have', \\\n",
    "                \"needn't've\": 'need not have', \"they'll've\": 'they will have', \"it's\": 'it is', \"haven't\": 'have not', \"didn't\": 'did not', \\\n",
    "                \"y'all'd\": 'you all would', \"needn't\": 'need not', \"who'll\": 'who will', \"wouldn't've\": 'would not have', \"when's\": 'when is', \\\n",
    "                \"will've\": 'will have', \"it'd've\": 'it would have', \"what'll\": 'what will', \"that'd've\": 'that would have', \\\n",
    "                \"y'all're\": 'you all are', \"let's\": 'let us', \"where've\": 'where have', \"o'clock\": 'oclock', \"when've\": 'when have', \\\n",
    "                \"what're\": 'what are', \"should've\": 'should have', \"you've\": 'you have', \"they're\": 'they are', \"aren't\": 'are not', \\\n",
    "                \"they've\": 'they have', \"it'd\": 'it would', \"i'll've\": 'i will have', \"they'd've\": 'they would have', \"you'll've\": 'you will have', \\\n",
    "                \"wouldn't\": 'would not', \"we'd\": 'we would', \"hadn't've\": 'had not have', \"weren't\": 'were not', \"i'd\": 'i would', \\\n",
    "                \"must've\": 'must have', \"what's\": 'what is', \"mustn't've\": 'must not have', \"what'll've\": 'what will have', \"ain't\": 'aint', \\\n",
    "                \"doesn't\": 'does not', \"we'll\": 'we will', \"i'd've\": 'i would have', \"we've\": 'we have', \"oughtn't\": 'ought not', \\\n",
    "                \"you're\": 'you are', \"who'll've\": 'who will have', \"shouldn't\": 'should not', \"can't've\": 'cannot have', \"i've\": 'i have', \\\n",
    "                \"couldn't've\": 'could not have', \"why've\": 'why have', \"what've\": 'what have', \"can't\": 'cannot', \"don't\": 'do not', \\\n",
    "                \"that'd\": 'that would', \"who's\": 'who is', \"would've\": 'would have', \"there'd\": 'there would', \"shouldn't've\": 'should not have', \\\n",
    "                \"y'all\": 'you all', \"mustn't\": 'must not', \"she'll\": 'she will', \"hadn't\": 'had not', \"won't've\": 'will not have', \\\n",
    "                \"why's\": 'why is', \"'cause\": 'because', \"wasn't\": 'was not', \"shan't've\": 'shall not have', \"ma'am\": 'madam', \"hasn't\": 'has not', \\\n",
    "                \"to've\": 'to have', \"how'll\": 'how will', \"oughtn't've\": 'ought not have', \"he'll've\": 'he will have', \"we'd've\": 'we would have', \\\n",
    "                \"won't\": 'will not', \"could've\": 'could have', \"isn't\": 'is not', \"she'll've\": 'she will have', \"we'll've\": 'we will have', \\\n",
    "                \"you'll\": 'you will', \"who've\": 'who have', \"there's\": 'there is', \"y'all've\": 'you all have', \"we're\": 'we are', \"i'll\": 'i will', \\\n",
    "                \"i'm\": 'i am', \"how's\": 'how is', \"she'd've\": 'she would have', \"sha'n't\": 'shall not', \"there'd've\": 'there would have', \\\n",
    "                \"he's\": 'he is', \"it'll've\": 'it will have', \"that's\": 'that is', \"y'all'd've\": 'you all would have', \"he'd've\": 'he would have', \\\n",
    "                \"how'd\": 'how did', \"where's\": 'where is', \"so's\": 'so as', \"she'd\": 'she would', \"mightn't've\": 'might not have'}\n",
    "\n",
    "# define list procedural words to remove as stop words\n",
    "procedural_words = [\"member\",\"members\",\"president\",\n",
    "    \"hon\",\"parliament\",\"house\",\"ask\",\"asked\",\"asks\",\"question\",\"questioned\",\"questions\",\"bills\",\"bill\",\n",
    "    \"party\",\"parties\",\"mp\",\"mps\",\"sir\",\"madam\",\"mr\",\"gentleman\",\"gentlemen\",\"lady\",\"ladies\",\n",
    "    \"speaker\",\"chair\",\"motion\",\"motions\",\"vote\",\"votes\",\"order\",\"yes\",\"deputy\",\"secretary\",\n",
    "    \"chairman\",\"chairwoman\",\n",
    "    \"america\",\"usa\",\"american\",\"americans\",\n",
    "    \"pursuant\",\"supply\",\"supplementary\",\"please\",\"friend\",\"s\",\n",
    "    \"clause\",\"amendment\",\"i\",\"ii\",\"iii\",\"section\",\"sections\", \"colleague\", \"colleagues\"]\n",
    "\n",
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c29461-0676-4865-b0d5-567d05155161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to clean speech text; cleaning functions drawn from Rheault and Cochrane\n",
    "def clean_text(text): \n",
    "    #convert to lower case\n",
    "    text = text.lower()\n",
    "    # replace contractions with whole words\n",
    "    text = reduce(lambda c, k_v: c.replace(*k_v), contractions_map.items(), text)\n",
    "    #remove excess whitespace characters\n",
    "    text = re.sub(r\"[\\t\\n\\r]\", \" \", text)\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # tokenize using nltk.word_tokenize\n",
    "    tokens = word_tokenize(text) \n",
    "    \n",
    "    # remove stop words, procedural words, words less than 2 characters long, blank spaces, digits\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in procedural_words and len(word)>2 and word!=\" \" and not word.isdigit()]\n",
    "    \n",
    "    # return cleaned text joined together as a string\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0400292-ad83-4f2a-8ff2-321785f5fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to extract just unique_id component of speakerid (from identifiers for Congress and chamber)\n",
    "def extract_speaker(speakerid): \n",
    "    speakerid=str(speakerid)\n",
    "    # each speaker has a unique, time invariant id that is characters 3-7 (inclusive) in speakerid identifier\n",
    "    return \"\".join([*speakerid][3:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77735639-486c-46f0-9ea4-0824b4cfde6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_102_to_108 = pd.read_csv(data_dir / \"house_102_to_108.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf76dcb-a7c6-4f08-b259-73d603f868f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get descriptive statistics on number of speeches per congress\n",
    "house_102_to_108.groupby([\"congress\", \"speakerid\"]).size().groupby(\"congress\").describe().to_csv(data_dir / \"speech_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc74cc1c-ede1-4814-83a6-58ace5dd44bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean speeches and extract unique speaker id for all of our speeches\n",
    "house_102_to_108[\"cleaned_speech\"] = house_102_to_108[\"speech\"].map(clean_text)\n",
    "house_102_to_108[\"cleaned_speaker\"] = house_102_to_108[\"speakerid\"].map(extract_speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789514a2-6847-44a1-9706-f09b8e1afb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data\n",
    "house_102_to_108 = house_102_to_108[[\"speech_id\", \"cleaned_speech\", \"cleaned_speaker\", \"party\", \"congress\"]]\n",
    "house_102_to_108.to_csv(data_dir / \"cleaned_house_102_to_108.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae3428a-bc8f-4411-b98e-58fd1f5e258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code drawn from Rheault and Cochrane (2020)\n",
    "class get_phrases(object):\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.file_path, \"r\") as file:\n",
    "            for i, line in enumerate(file):\n",
    "                # skip first line - is header with column names\n",
    "                if i == 0: continue\n",
    "                \n",
    "                # split line by tab character\n",
    "                line_split = line.split(\"\\t\")\n",
    "                \n",
    "                # yield speech text as a list of words\n",
    "                text = line_split[1].replace(\"\\n\",\"\") \n",
    "                yield text.split()\n",
    "\n",
    "                \n",
    "class get_document(object):\n",
    "\n",
    "    def __init__(self, file_path, bigram, trigram):\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.file_path, \"r\") as file:\n",
    "            for i, line in enumerate(file):\n",
    "                \n",
    "                # skip first line - is header with column names\n",
    "                if i == 0: continue\n",
    "                \n",
    "                # split line by tab character\n",
    "                ls = line.split(\"\\t\")\n",
    "                \n",
    "                # get speech text and get bigrams and trigrams\n",
    "                text = ls[1].replace(\"\\n\",\"\")\n",
    "                tokens = text.split()\n",
    "                self.words = self.trigram[self.bigram[tokens]]\n",
    "                \n",
    "                # create legislator-congress tag, party is included for later analysis but has no effect\n",
    "                speaker = ls[2]\n",
    "                party = ls[3]\n",
    "                congress = ls[4].replace(\"\\n\",\"\")\n",
    "                tags = [f\"{speaker}_{party}_{congress}\"]\n",
    "                self.tags = tags\n",
    "                \n",
    "                # yield the tagged document\n",
    "                yield TaggedDocument(self.words, self.tags)\n",
    "               \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88943558-a81f-4027-a19e-ff7a66d7f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(get_phrases(data_dir / \"cleaned_house_102_to_108.txt\"))\n",
    "bigram = Phraser(phrases)\n",
    "tphrases = Phrases(bigram[get_phrases(data_dir / \"cleaned_house_102_to_108.txt\")])\n",
    "trigram = Phraser(tphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6725ce-02e3-47da-ba24-270849dff40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model\n",
    "model = Doc2Vec(vector_size=200, window=20, min_count=50, workers=8, epochs=20, alpha=0.025)\n",
    "model.build_vocab(get_document(data_dir / \"cleaned_house_102_to_108.txt\", bigram=bigram, trigram=trigram), min_count=50)\n",
    "model.train(get_document(data_dir / \"cleaned_house_102_to_108.txt\", bigram=bigram, trigram=trigram), \n",
    "             total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2781cfb0-ce2f-468b-b764-d3ce06e0c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../models/main_model_102_108\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1da3b-d39c-4f59-b3d5-c63d9bfbbe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"../models/main_model_102_108\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c93b7e-e35b-4436-bc50-208cf733ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of tags for the model\n",
    "speaker_tags = model.dv.index_to_key\n",
    "\n",
    "# for each tag, get the model's tag-embedding (the main parameters of interest)\n",
    "embeds = np.array([model.dv[tag] for tag in speaker_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ad61b3-fa2e-4daf-9d5a-2e6e128d9715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a 2D PCA on the legislator-session embeddings and turn into a dataframe\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "pca_df = pd.DataFrame(pca.fit_transform(embeds), columns=[\"pc1\", \"pc2\"])\n",
    "\n",
    "# add column for legislator- session tage\n",
    "pca_df[\"tag\"] = speaker_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d2e8f-4a48-491b-b968-f6115e6aafbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split tage into ID, party, congress components\n",
    "pca_df[[\"unique_id\",\"party\",\"congress\"]] = pca_df[\"tag\"].str.split('_', n=2, expand=True)\n",
    "\n",
    "# remake speaker id column\n",
    "pca_df[\"speakerid\"] = pca_df[\"congress\"] + pca_df[\"unique_id\"] \n",
    "\n",
    "speakers = pd.read_csv(data_dir / \"speakers_102_to_108.csv\")\n",
    "\n",
    "# merged with speakerid to get speaker information\n",
    "pca_df = pca_df.merge(speakers, how=\"left\", on=\"speakerid\")\n",
    "\n",
    "# save \n",
    "pca_df.to_csv(data_dir / \"pca_102_108.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ef513-fcd9-4210-bb04-68d6605bfe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist=[]\n",
    "# get words and how many times they appear \n",
    "for word in model.wv.key_to_index.keys():\n",
    "    wordlist.append((word, model.wv.get_vecattr(word, \"count\")))\n",
    "\n",
    "# get words that appear more than 100 times in the corpus\n",
    "vocab = [word for word,count in wordlist if count>100]\n",
    "\n",
    "word_pca = np.zeros((len(vocab), 2))\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    word_pca[i, :] = pca.transform(model.wv[word].reshape(1,-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ffaba3-b172-4a09-8fef-4036f39d7fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and save 20 words that map as lowest (most liberal) on pc1 (ideology axis)\n",
    "pd.DataFrame({'word': sorted_vocab, 'pc1': S.T[0], 'pc2':S.T[1]}).sort_values(\"pc1\").head(20).to_csv(data_dir/\"most_liberal_words.csv\")\n",
    "\n",
    "# get and save 20 words that map as highest (most conservative) on pc1 (ideology axis)\n",
    "pd.DataFrame({'word': sorted_vocab, 'pc1': S.T[0], 'pc2':S.T[1]}).sort_values(\"pc1\").head(20).to_csv(data_dir/\"most_conservative_words.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:new_env]",
   "language": "python",
   "name": "conda-env-new_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
